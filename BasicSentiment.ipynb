{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_good = [\"I like machine learning\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"I don't hate you\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data_good = [TaggedDocument(words=word_tokenize(_d.lower()), tags=['GOOD']) for i, _d in enumerate(data_good)]\n",
    "\n",
    "data_bad = [\"I don't like machine learning\",\n",
    "        \"I hate coding in python\",\n",
    "        \"I really hate building chatbots\",\n",
    "        \"they chat really bad\"]\n",
    "\n",
    "tagged_data_bad = [TaggedDocument(words=word_tokenize(_d.lower()), tags=['BAD']) for i, _d in enumerate(data_bad)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "tagged_data = tagged_data_good+ tagged_data_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 38597.89it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 39988.07it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63336.81it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 67049.26it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 67529.04it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 59634.65it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52648.17it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 57368.90it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 70295.60it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 69775.85it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 56594.81it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 36019.79it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 58074.98it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54629.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 53926.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 66225.85it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 57985.77it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 57195.05it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 58890.38it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54629.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 65084.03it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63019.59it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 60787.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54629.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52648.17it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63336.81it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 60787.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 55512.85it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 58434.58it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 56257.43it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 25802.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=1, \n",
    "                     vector_size=300, \n",
    "                     negative=5, \n",
    "                     hs=0, \n",
    "                     min_count=2, \n",
    "                     sample = 0, \n",
    "                     workers=cores)\n",
    "\n",
    "model_dbow.build_vocab([x for x in tqdm(tagged_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(tagged_data)]), total_examples=len(tagged_data), epochs=10)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data ['i', 'do', \"n't\", 'hate']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BAD', 0.9999945759773254), ('GOOD', 0.999988853931427)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I don't hate\".lower())\n",
    "\n",
    "print('test data', test_data)\n",
    "\n",
    "# model.most_similar('learning')\n",
    "\n",
    "v = model.infer_vector(test_data)\n",
    "model.docvecs.most_similar([v], topn=len(model.docvecs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('love', 0.999557375907898),\n",
       " ('coding', 0.9995526075363159),\n",
       " ('machine', 0.9995464086532593),\n",
       " ('learning', 0.999544084072113),\n",
       " ('chat', 0.9995251893997192),\n",
       " ('like', 0.9995152950286865),\n",
       " ('python', 0.9995043873786926),\n",
       " ('well', 0.999496340751648),\n",
       " ('amagingly', 0.9994792938232422),\n",
       " ('bad', 0.9994626045227051)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "        sents = tagged_docs\n",
    "        targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "        return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
